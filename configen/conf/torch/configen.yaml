configen:
  # output directory
  output_dir: ${hydra:runtime.cwd}

  header: |
    # Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
    #
    # Generated by configen, do not edit.
    # See https://github.com/facebookresearch/hydra/tree/master/tools/configen
    # fmt: off
    # isort:skip_file
    # flake8: noqa

  module_path_pattern: 'hydra_configs/{{module_path}}.py'

  # list of modules to generate configs for
  modules:
    - name: torch.optim.adadelta
      classes:
        - Adadelta

    - name: torch.optim.adagrad
      classes:
        - Adagrad

    - name: torch.optim.adam
      classes:
        - Adam

    - name: torch.optim.adamw
      classes:
        - AdamW

    - name: torch.optim.sparse_adam
      classes:
        - SparseAdam

    - name: torch.optim.adamax
      classes:
        - Adamax

    - name: torch.optim.asgd
      classes:
        - ASGD

    - name: torch.optim.sgd
      classes:
        - SGD

    - name: torch.optim.rprop
      classes:
        - Rprop

    - name: torch.optim.rmsprop
      classes:
        - RMSprop

    - name: torch.optim.lbfgs
      classes:
        - LBFGS

    - name: torch.optim.lr_scheduler
      classes:
        - LambdaLR
        - MultiplicativeLR
        - StepLR
        - MultiStepLR
        - ExponentialLR
        - CosineAnnealingLR
        - ReduceLROnPlateau
        - CyclicLR
        - CosineAnnealingWarmRestarts
        - OneCycleLR

    - name: torch.utils.data.dataloader
      classes:
        - DataLoader

    - name: torch.utils.data.dataset
      classes:
        - Dataset
        - IterableDataset
        - TensorDataset
        - ConcatDataset
        - ChainDataset
        - Subset

    - name: torch.utils.data.sampler
      classes:
        - Sampler
        - SequentialSampler
        - RandomSampler
        - SubsetRandomSampler
        - WeightedRandomSampler
        - BatchSampler

    - name: torch.utils.data.distributed
      classes:
        - DistributedSampler
