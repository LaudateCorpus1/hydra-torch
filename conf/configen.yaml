configen:
  # output directory
  output_dir: ${hydra:runtime.cwd}

  header: |
    # Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
    #
    # Generated by configen, do not edit.
    # See https://github.com/facebookresearch/hydra/tree/master/tools/configen
    # fmt: off
    # isort:skip_file
    # flake8: noqa

  module_path_pattern: 'config/{{module_path}}.py'

  # list of modules to generate configs for
  modules:
    - name: torch.optim.adadelta
      # for each module, a list of classes
      classes:
        - Adadelta

    - name: torch.optim.adagrad
      # for each module, a list of classes
      classes:
        - Adagrad

    - name: torch.optim.adam
      # for each module, a list of classes
      classes:
        - Adam

    - name: torch.optim.adamw
      # for each module, a list of classes
      classes:
        - AdamW

    - name: torch.optim.sparse_adam
      # for each module, a list of classes
      classes:
        - SparseAdam 

    - name: torch.optim.adamax
      # for each module, a list of classes
      classes:
        - Adamax

    - name: torch.optim.asgd
      # for each module, a list of classes
      classes:
        - ASGD 

    - name: torch.optim.sgd
      # for each module, a list of classes
      classes:
        - SGD 

    - name: torch.optim.rprop
      # for each module, a list of classes
      classes:
        - Rprop 

    - name: torch.optim.rmsprop
      # for each module, a list of classes
      classes:
        - RMSprop 

    - name: torch.optim.lbfgs
      # for each module, a list of classes
      classes:
        - LBFGS 

    - name: torch.optim.lr_scheduler
      classes:
        # Schedulers
        - LambdaLR
        - MultiplicativeLR
        - StepLR
        - MultiStepLR
        - ExponentialLR
        - CosineAnnealingLR
        - ReduceLROnPlateau
        - CyclicLR
        - CosineAnnealingWarmRestarts
        - OneCycleLR
